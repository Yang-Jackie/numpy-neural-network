{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, layers: list[int], random_weights = True):\n",
    "        n = len(layers)\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        for i in range(1, n):\n",
    "            self.b.append(np.zeros(layers[i]))\n",
    "            if random_weights: self.W.append(np.random.randn(layers[i-1], layers[i]) * 2 / math.sqrt(layers[i]))\n",
    "            else: self.W.append(np.zeros((layers[i-1], layers[i])))\n",
    "\n",
    "    def clear(self):\n",
    "        self.W = [np.zeros_like(wi) for wi in self.W]\n",
    "        self.b = [np.zeros_like(bi) for bi in self.b]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, str):\n",
    "            if idx.lower() == 'w': return self.W\n",
    "            elif idx.lower() == 'b': return self.b\n",
    "            else:\n",
    "                raise KeyError(\"There is no key named \" + idx)\n",
    "        if isinstance(idx, tuple):\n",
    "            type, i = idx\n",
    "            return self[type][i]\n",
    "        else: return self.W[idx], self.b[idx]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list[int]):\n",
    "        self.depth = len(layers)\n",
    "        self.parameters = Parameters(layers)\n",
    "        self.grad = Parameters(layers, random_weights=False)\n",
    "        self.cache = []\n",
    "\n",
    "    def __call__(self, x: np.ndarray, flatten = False):\n",
    "        if flatten:\n",
    "            if x.ndim == 3: x = x.reshape(x.shape[0], -1)\n",
    "            elif x.ndim == 2: x = x.reshape(-1)\n",
    "        \n",
    "        if x.ndim == 1: x = x.reshape(1, -1)\n",
    "\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(f\"x must be 2D (ndim=2). Current shape is {x.shape}\")\n",
    "\n",
    "        w = self.parameters['w']\n",
    "        b = self.parameters['b']\n",
    "        self.cache = []\n",
    "        for i in range(0, self.depth-1):\n",
    "            self.cache.append(x)\n",
    "            x = x @ w[i] + b[i]\n",
    "            #Apply ReLU\n",
    "            x[x < 0] = 0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def clear_grad(self):\n",
    "        self.grad.clear()\n",
    "    \n",
    "    def backprop(self, grad_last_layer):\n",
    "        \"\"\"\n",
    "        Calculate the gradient for a batch and store it in self.grad\n",
    "\n",
    "        Parameters:\n",
    "            grad_last_layer: (batch, classes) representing gradient of the last layer across the whole batch\n",
    "        \"\"\"\n",
    "        n = self.depth\n",
    "        \n",
    "        w = self.parameters['w']\n",
    "        dz = grad_last_layer\n",
    "        dw = self.grad['w']\n",
    "        db = self.grad['b']\n",
    "        for i in range(n-2, -1, -1):\n",
    "            dw[i] = self.cache[i].T @ dz\n",
    "            db[i] = dz.sum(axis=0)\n",
    "            \n",
    "            dz = dz @ w[i].T\n",
    "            dz = dz * (self.cache[i] > 0)\n",
    "\n",
    "\n",
    "    def update_with_grad(self, learning_rate = 0.01):\n",
    "        #Basic optimizer\n",
    "        w = self.parameters['w']\n",
    "        b = self.parameters['b']\n",
    "        dw = self.grad['w']\n",
    "        db = self.grad['b']\n",
    "        for i in range(0, self.depth - 1):\n",
    "            w[i] -= learning_rate * dw[i]\n",
    "            b[i] -= learning_rate * db[i]\n",
    "        \n",
    "        self.clear_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71514191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits: np.ndarray, y: np.array, return_grad = False):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss on logits (model output)\n",
    "\n",
    "    Parameters:\n",
    "        logits (np.ndarray): 2D (batch, classes) array representing logits.\n",
    "        y (np.ndarray):\n",
    "            if 1D (batch,): treated as the correct class indices for each batch item\n",
    "            if 2D (batch, classes): treated as the one-hot vector for each batch item\n",
    "        return_grad:\n",
    "            if True: additionally return the gradient of the last layer w.r.t loss function\n",
    "            if False: default behaviour\n",
    "    \n",
    "    Returns:\n",
    "        when return_grad == False: return a 1D (batch,) np.ndarray representing the loss for each batch item\n",
    "        when return_grad == True: return (np.ndarray, np.ndarray) where the second entry is (batch, classes) representing gradients\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if logits.ndim == 1: logits = logits.reshape(1, -1)\n",
    "\n",
    "    B = logits.shape[0]\n",
    "\n",
    "    if y.ndim == 2:\n",
    "        if not np.all((y == 0) | (y == 1)):\n",
    "            raise ValueError(\"y must be one-hot encoded (0/1).\")\n",
    "\n",
    "        if not np.all(y.sum(axis=1) == 1):\n",
    "            raise ValueError(\"Each row of y must contain exactly one 1.\")\n",
    "        \n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exp_z = np.exp(logits)\n",
    "    probs = exp_z/exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    loss = -logits[np.arange(B), y] + np.log(np.sum(exp_z, axis=1))\n",
    "\n",
    "    if return_grad == False: return loss\n",
    "    \n",
    "    grad = probs\n",
    "    grad[np.arange(B), y] -= 1\n",
    "    grad /= B\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad011ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\")/255.0\n",
    "x_test = x_test.astype(\"float32\")/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a01f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels):\n",
    "    n = len(images)\n",
    "    cols = 4\n",
    "    rows = math.ceil(n / 4)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(2*cols, 2*rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        if (i < n):\n",
    "            axes[i].imshow(images[i], cmap='gray')\n",
    "            axes[i].set_title(str(labels[i]))\n",
    "        axes[i].axis(\"off\")\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_random_images(n):\n",
    "    indices = np.random.choice(60000, size=n, replace=False)\n",
    "\n",
    "    images = x_train[indices]\n",
    "    labels = y_train[indices]\n",
    "\n",
    "    show_images(images, labels)\n",
    "\n",
    "show_random_images(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901707d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "mean = x_train.mean(axis=0, keepdims=True)\n",
    "std  = x_train.std(axis=0, keepdims=True) + 1e-7\n",
    "\n",
    "X_train = (x_train - mean) / std\n",
    "X_test  = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab45157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(batch_size = 256, x = x_train, y = y_train, shuffle = True):\n",
    "    n = len(x)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(n)\n",
    "        shuffled_x = x[indices]\n",
    "        shuffled_y = y[indices]\n",
    "\n",
    "    for l in range(0, n, batch_size):\n",
    "        r = min(l + batch_size, n)\n",
    "        yield shuffled_x[l:r], shuffled_y[l:r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f70927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    return np.sum(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(layers=[784, 512, 256, 128, 10])\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}/{epochs}:\", end=\"\")\n",
    "\n",
    "    samples_processed = 0\n",
    "    total_test_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "    for x, y in generate_batches():\n",
    "        #Training:\n",
    "        y_pred = model(x)\n",
    "        loss, grad = cross_entropy_loss(y_pred, y, True)\n",
    "        \n",
    "        total_train_loss += loss.sum()\n",
    "\n",
    "        model.backprop(grad)\n",
    "        model.update_with_grad()\n",
    "\n",
    "        samples_processed += len(x)\n",
    "        print(f\"\\rEpoch {i}/{epochs}: {samples_processed}/{len(x_train)}\", end=\"\")\n",
    "    \n",
    "    y_test_pred = model(x_test)\n",
    "    total_test_accuracy += accuracy(y_test_pred, y_test)\n",
    "\n",
    "    print(\"\\n-------------------------------\")\n",
    "    print(f\"Loss: {total_train_loss / len(x_train)}\")\n",
    "    print(f\"Accuracy: {total_test_accuracy / len(x_test)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
